game:
  ws_url: "ws://localhost:4000/ws"
  http_url: "http://localhost:4000"
  default_room_code: null
  default_player_name: "RL-Bot-Quick"
  connection_timeout: 30.0
  reconnect_attempts: 3
  reconnect_delay: 5.0

state_representation:
  representation_type: "feature_vector"
  grid_resolution: 16
  history_length: 2
  normalize_coordinates: true
  include_velocity: true
  include_health: true
  include_ammunition: true
  feature_vector_size: 32

action_space:
  action_type: "discrete"
  discrete_actions:
    - "no_action"
    - "move_left"
    - "move_right"
    - "jump"
    - "shoot"
  continuous_movement_range: [-1.0, 1.0]
  continuous_aim_range: [0.0, 1.0]
  action_repeat: 1

reward:
  primary_function: "win_loss"
  secondary_functions:
    - "survival"
  weights: [0.8, 0.2]
  horizon_weights:
    short_term: 0.7
    medium_term: 0.2
    long_term: 0.1
  normalization: "min_max"
  reward_clipping: [-5.0, 5.0]
  exploration_bonus: 0.05

training:
  algorithm: "DQN"
  total_timesteps: 100000
  learning_rate: 0.001
  batch_size: 32
  buffer_size: 10000
  gamma: 0.95
  tau: 0.005
  exploration_fraction: 0.2
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.1
  train_freq: 4
  gradient_steps: 1
  target_update_interval: 1000
  save_freq: 5000
  eval_freq: 2500
  eval_episodes: 5
  log_interval: 50
  device: "auto"

cohort_training:
  cohort_size: 2
  enemy_count_range: [1, 2]
  opponent_selection: "random"
  difficulty_progression: false
  rules_bot_weight: 0.5
  generation_weights: [0.6, 0.4]

evaluation:
  evaluation_episodes: 20
  evaluation_opponents:
    - "rules_bot"
    - "previous_generation"
  metrics_to_track:
    - "win_rate"
    - "average_reward"
  statistical_significance: 0.1
  min_improvement_threshold: 0.1

model:
  network_architecture: "mlp"
  hidden_layers: [128, 64]
  activation: "relu"
  dropout_rate: 0.0
  batch_norm: false
  max_generations: 10
  keep_best_n: 5
  knowledge_transfer_method: "weight_initialization"

speed_control:
  training_speed_multiplier: 20.0
  evaluation_speed_multiplier: 1.0
  headless_mode: true
  max_parallel_episodes: 2
  batch_episode_size: 8

logging:
  log_level: "INFO"
  log_to_file: true
  log_file_path: "logs/rl_bot_system_quick.log"
  use_tensorboard: true
  tensorboard_log_dir: "logs/tensorboard_quick"
  use_wandb: false
  wandb_project: "rl-bot-system-quick"
  wandb_entity: null

data_dir: "data"
models_dir: "data/models"
replays_dir: "data/replays"
metrics_dir: "data/metrics"
logs_dir: "logs"