game:
  ws_url: "ws://localhost:4000/ws"
  http_url: "http://localhost:4000"
  default_room_code: null
  default_player_name: "RL-Bot"
  connection_timeout: 30.0
  reconnect_attempts: 3
  reconnect_delay: 5.0

state_representation:
  representation_type: "feature_vector"
  grid_resolution: 32
  history_length: 4
  normalize_coordinates: true
  include_velocity: true
  include_health: true
  include_ammunition: true
  feature_vector_size: 64

action_space:
  action_type: "discrete"
  discrete_actions:
    - "no_action"
    - "move_left"
    - "move_right"
    - "jump"
    - "shoot"
    - "move_left_shoot"
    - "move_right_shoot"
    - "jump_shoot"
  continuous_movement_range: [-1.0, 1.0]
  continuous_aim_range: [0.0, 1.0]
  action_repeat: 1

reward:
  primary_function: "health_differential"
  secondary_functions:
    - "aim_accuracy"
    - "survival"
  weights: [0.7, 0.2, 0.1]
  horizon_weights:
    short_term: 0.5
    medium_term: 0.3
    long_term: 0.2
  normalization: "z_score"
  reward_clipping: [-10.0, 10.0]
  exploration_bonus: 0.01

training:
  algorithm: "PPO"
  total_timesteps: 1000000
  learning_rate: 0.0003
  batch_size: 64
  buffer_size: 100000
  gamma: 0.99
  tau: 0.005
  exploration_fraction: 0.1
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.02
  train_freq: 4
  gradient_steps: 1
  target_update_interval: 10000
  n_steps: 2048
  n_epochs: 10
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  save_freq: 10000
  eval_freq: 5000
  eval_episodes: 10
  log_interval: 100
  device: "auto"

cohort_training:
  cohort_size: 3
  enemy_count_range: [1, 3]
  opponent_selection: "weighted"
  difficulty_progression: true
  rules_bot_weight: 0.3
  generation_weights: [0.4, 0.3, 0.2]

evaluation:
  evaluation_episodes: 100
  evaluation_opponents:
    - "rules_bot"
    - "previous_generation"
    - "random"
  metrics_to_track:
    - "win_rate"
    - "average_reward"
    - "episode_length"
    - "strategic_diversity"
  statistical_significance: 0.05
  min_improvement_threshold: 0.05

model:
  network_architecture: "mlp"
  hidden_layers: [256, 256]
  activation: "relu"
  dropout_rate: 0.0
  batch_norm: false
  max_generations: 50
  keep_best_n: 10
  knowledge_transfer_method: "weight_initialization"

speed_control:
  training_speed_multiplier: 10.0
  evaluation_speed_multiplier: 1.0
  headless_mode: true
  max_parallel_episodes: 4
  batch_episode_size: 16

logging:
  log_level: "INFO"
  log_to_file: true
  log_file_path: "logs/rl_bot_system.log"
  use_tensorboard: true
  tensorboard_log_dir: "logs/tensorboard"
  use_wandb: false
  wandb_project: "rl-bot-system"
  wandb_entity: null

data_dir: "data"
models_dir: "data/models"
replays_dir: "data/replays"
metrics_dir: "data/metrics"
logs_dir: "logs"