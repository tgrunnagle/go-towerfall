game:
  ws_url: "ws://localhost:4000/ws"
  http_url: "http://localhost:4000"
  default_room_code: null
  default_player_name: "RL-Bot-GPU"
  connection_timeout: 30.0
  reconnect_attempts: 3
  reconnect_delay: 5.0

state_representation:
  representation_type: "feature_vector"
  grid_resolution: 64  # Increased for GPU
  history_length: 8    # Increased for GPU
  normalize_coordinates: true
  include_velocity: true
  include_health: true
  include_ammunition: true
  feature_vector_size: 128  # Increased for GPU

action_space:
  action_type: "discrete"
  discrete_actions:
    - "no_action"
    - "move_left"
    - "move_right"
    - "jump"
    - "shoot"
    - "move_left_shoot"
    - "move_right_shoot"
    - "jump_shoot"
    - "move_left_jump"
    - "move_right_jump"
    - "dodge_left"
    - "dodge_right"
  continuous_movement_range: [-1.0, 1.0]
  continuous_aim_range: [0.0, 1.0]
  action_repeat: 1

reward:
  primary_function: "health_differential"
  secondary_functions:
    - "aim_accuracy"
    - "survival"
    - "strategic_positioning"
  weights: [0.6, 0.2, 0.1, 0.1]
  horizon_weights:
    short_term: 0.4
    medium_term: 0.4
    long_term: 0.2
  normalization: "z_score"
  reward_clipping: [-15.0, 15.0]
  exploration_bonus: 0.005

training:
  algorithm: "PPO"
  total_timesteps: 5000000  # Increased for GPU
  learning_rate: 0.0003
  batch_size: 256          # Increased for GPU
  buffer_size: 1000000     # Increased for GPU
  gamma: 0.99
  tau: 0.005
  exploration_fraction: 0.1
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.02
  train_freq: 4
  gradient_steps: 1
  target_update_interval: 10000
  n_steps: 4096            # Increased for GPU
  n_epochs: 20             # Increased for GPU
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  save_freq: 25000
  eval_freq: 10000
  eval_episodes: 20
  log_interval: 50
  device: "auto"           # Auto-detect GPU

cohort_training:
  cohort_size: 5           # Increased for GPU
  enemy_count_range: [1, 4]
  opponent_selection: "weighted"
  difficulty_progression: true
  rules_bot_weight: 0.2
  generation_weights: [0.4, 0.3, 0.2, 0.1]

evaluation:
  evaluation_episodes: 200  # Increased for GPU
  evaluation_opponents:
    - "rules_bot"
    - "previous_generation"
    - "random"
    - "best_generation"
  metrics_to_track:
    - "win_rate"
    - "average_reward"
    - "episode_length"
    - "strategic_diversity"
    - "reaction_time"
  statistical_significance: 0.05
  min_improvement_threshold: 0.03

model:
  network_architecture: "mlp"
  hidden_layers: [512, 512, 256]  # Larger network for GPU
  activation: "relu"
  dropout_rate: 0.1
  batch_norm: true
  max_generations: 100     # Increased for GPU
  keep_best_n: 20
  knowledge_transfer_method: "weight_initialization"

speed_control:
  training_speed_multiplier: 20.0  # Faster with GPU
  evaluation_speed_multiplier: 1.0
  headless_mode: true
  max_parallel_episodes: 8         # Increased for GPU
  batch_episode_size: 32           # Increased for GPU

logging:
  log_level: "INFO"
  log_to_file: true
  log_file_path: "logs/rl_bot_system_gpu.log"
  use_tensorboard: true
  tensorboard_log_dir: "logs/tensorboard_gpu"
  use_wandb: true          # Enable for GPU training
  wandb_project: "rl-bot-system-gpu"
  wandb_entity: null

data_dir: "data"
models_dir: "data/models"
replays_dir: "data/replays"
metrics_dir: "data/metrics"
logs_dir: "logs"